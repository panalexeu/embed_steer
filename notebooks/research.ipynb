{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2e5b39-9999-4b50-ada3-ac8af914acab",
   "metadata": {},
   "source": [
    "### Embedding Steering\n",
    "\n",
    "The idea of embedding steering is influenced by attention steering in LLMs and hypothetical document generation in RAG pipelines [paper ref here].  \n",
    "In the first iteration, I propose not to influence any inner layers of the embedding model, but rather to steer or influence the final embedding representation of some text.\n",
    "\n",
    "Suppose we have an embedding model that performs well in semantic understanding of text but is not fine-tuned for QA or queryâ€“passage retrieval.\n",
    "\n",
    "As a possible option to fix this problem without fine-tuning, I propose:  \n",
    "\n",
    "1. Use an LLM to generate a set of possible queries for a passage;  \n",
    "2. Embed the original passage and the set of possible queries;  \n",
    "3. Use some method to fuse the embeddings of the queries and the passage (weighted averaging, a custom function, etc.);  \n",
    "4. Store the original passage and the fused embedding for retrieval.  \n",
    "\n",
    "To evaluate the proposed method, I suggest using the Natural Questions dataset or MS MARCO.\n",
    "\n",
    "As a metric, I propose using [nDCG@10](https://en.wikipedia.org/wiki/Discounted_cumulative_gain), the same metric used in the MTEB evaluation benchmark for [NQ](https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/) and [MS MARCO](https://github.com/microsoft/MSMARCO-Passage-Ranking). The metric evaluates relevant passage ranking and has the following formula:  \n",
    "\n",
    "$$\n",
    "\\text{DCG}_p = \\sum_{i=1}^{p} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{nDCG}_p = \\frac{\\text{DCG}_p}{\\text{IDCG}_p},\n",
    "$$\n",
    "\n",
    "where IDCG is the ideal discounted cumulative gain.\n",
    "\n",
    "I would prefer to use MS MARCO as the evaluation dataset since `sentence-transformers` provides pfine-tuned open-source [models](https://sbert.net/docs/sentence_transformer/pretrained_models.html) on this dataset. This makes it possible to compare the fine-tuned version with the embedding steering method applied to the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f14f3a-697f-4f9e-a5ba-b5831728023a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed-steer",
   "language": "python",
   "name": "embed-steer"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
